{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9258ab-cf06-44fa-a94e-aa79107afd60",
   "metadata": {},
   "source": [
    "#### Linear regression with multiple variables is also known as \"multivariate linear regression\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8acdfe2-5793-4bf5-a5a4-a4686ab7a810",
   "metadata": {},
   "source": [
    " ![Description](images/a.png)\n",
    " #### The multivariable form of the hypothesis function accommodating these multiple features is as follows:\n",
    " ![b](images/b.png)\n",
    " #### In order to develop intuition about this function, we can think about $\\theta_0$ or $Slope_1$ as the basic price of a house, $\\theta_{1}$ or $Slope_2$ as the price per square meter, $\\theta_2$ as the price per floor, etc. $x_1$  will be the number of square meters in the house, $x_2$  the number of floors, etc.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c5b0d-0c40-4c5b-86ad-6e5a9adbcd74",
   "metadata": {},
   "source": [
    "#### Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:\n",
    "![c](images/c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21703f96-490e-4d24-952f-1f8fb99b6d1f",
   "metadata": {},
   "source": [
    "#### We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    ">#### The goal is to get all input variables into roughly one of these ranges, give or take a few."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56f546-91c6-4312-b5f2-d63a3c663dce",
   "metadata": {},
   "source": [
    "#### Two techniques to help with this are **feature scaling** and **mean normalization.** \n",
    ">#### Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1.\n",
    ">#### Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. \n",
    "\n",
    "![normalization](images/normalizing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc248a35-046c-47b4-ab27-bd3fe26dcd17",
   "metadata": {},
   "source": [
    "#### Where $\\mu_i$ s the average of all the values for feature (i) and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309dc8c-9511-4c0d-9f60-6d436677bd55",
   "metadata": {},
   "source": [
    "#### If $X^TX$ is non-invertible,  the common causes might be having :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea088789-9ea7-40dd-bee5-db988c2bec63",
   "metadata": {},
   "source": [
    ">#### Redundant features, where two features are very closely related (i.e. they are linearly dependent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b5ac9b-5594-4a2c-83e0-83f06eb38df1",
   "metadata": {},
   "source": [
    ">#### Too many features (e.g. m â‰¤ n). In this case, delete some features or use \"regularization\" (to be explained in a later lesson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a05f1-a460-4132-9a82-2ad33518f18d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
