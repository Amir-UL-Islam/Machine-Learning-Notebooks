{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepocessing with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main task for machine learning engineers is to first analyze the data for viable trends, then create an efficient input pipeline for training a model.This process involves using libraries like NumPy and pandas for handling data, along with machine learning frameworks like TensorFlow for creating the model and input pipeline. \n",
    "#### The scikit-learn library includes tools for data preprocessing and data mining. It is imported in Python via the statement \n",
    "> **import sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When data can take on any range of values, it makes it difficult to interpret. Therefore, data scientists will convert the data into a standard format to make it easier to understand. **The standard format refers to data that has 0 mean and unit variance (i.e. standard deviation = 1), and the process of converting data into this format is called data standardization.**\n",
    "\n",
    "#### Data standardization is a relatively simple process. For each data value, x, we subtract the overall mean of the data, μ, then divide by the overall standard deviation, σ. The new value, z, represents the standardized data value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy and scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The array’s rows represent individual data observations, while each column represents a particular feature of the data, i.e. the same format as a spreadsheet data table.\n",
    "\n",
    "#### The scikit-learn data preprocessing module is called **sklearn.preprocessing**. One of the functions in this module, scale, applies data standardization to a given axis of a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n",
      "array([[-0.16552118, -1.06904497, -0.1393466 ],\n",
      "       [ 1.4896906 ,  0.26726124,  1.60248593],\n",
      "       [-1.40693001, -1.06904497, -1.53281263],\n",
      "       [-0.57932412,  1.60356745, -0.1393466 ],\n",
      "       [ 0.66208471,  0.26726124,  0.2090199 ]])\n",
      "\n",
      "array([ 0., -0.,  0.])\n",
      "\n",
      "array([1., 1., 1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# defined pizza data\n",
    "pizza_data = np.array([[2100,   10,  800],\n",
    "       [2500,   11,  850],\n",
    "       [1800,   10,  760],\n",
    "       [2000,   12,  800],\n",
    "       [2300,   11,  810]])\n",
    "# Newline to separate print statements\n",
    "print('{}\\n'.format(repr(pizza_data)))\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "# Standardizing each column of pizza_data\n",
    "col_standardized = scale(pizza_data)\n",
    "print('{}\\n'.format(repr(col_standardized)))\n",
    "\n",
    "# Column means (rounded to nearest thousandth)\n",
    "col_means = col_standardized.mean(axis=0).round(decimals=3)\n",
    "print('{}\\n'.format(repr(col_means)))\n",
    "\n",
    "# Column standard deviations\n",
    "col_stds = col_standardized.std(axis=0)\n",
    "print('{}\\n'.format(repr(col_stds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We normally standardize the data independently across each feature of the data array. This way, we can see how many standard deviations a particular observation's feature value is from the mean.\n",
    "#### If for some reason we need to standardize the data across rows, rather than columns, we can set the axis keyword argument in the scale function to 1. This may be the case when analyzing data within observations, rather than within a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
